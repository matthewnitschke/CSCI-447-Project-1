\documentclass[11pt]{article} % Font size
\usepackage[a4paper, total={6in, 10.5in}]{geometry} % Margins

\usepackage{graphicx}
\usepackage[simplified]{pgf-umlcd}
\usepackage{tikz}


\title{\textbf{CSCI 447 - Project 1}}
\author{Wilson Harris | Matthew Nitschke | Alex Abrahamson }
\date{September 11, 2017}

\begin{document}
\maketitle

\section{Description of the Problem}
The problem is the conversion of data from the UCI ML repository (which is in .csv format) to ARFF format, which is the format required by WEKA. This involves having the software find out how many attributes are in the .csv file, what the attributes are, and how to rearrange each individual line to fit the ARFF format. The largest problem for the utility is to determine the datatype of each attribute that is listed in the first row of the .csv file. While the first row of a .csv file lists the name of each attribute, it does not list the datatype. The utility must look at each column of each row, and determine the datatype from there. This is done through command-line arguments, where the user enters the types when invoking the utility. If the utility receives its command-line arguments properly and the .csv file is properly formatted, then the end result is a well-formed ARFF file that has lost no data in the translation.

\section{ARFF Converter}

\subsection{Design}
Our ARFF converter is a simple command line application written in javascript and run via the javascript runtime environment Node.js. The parameters of the program ask for the filename of the .cvs file, and a list of attribute types. Once run, the program parses out the filename and attribute types. It then loops through each attribute type looking for types of "enum" or "date", where if encountered asks the user for enum properties or date format. After the arguments have been parsed, arffConvert reads the desired .csv file and extracts the header line. This line is split into an array by its commas.  Then it is looped through, appending its name and correlating attribute type at each iteration. Finally, the data of the csv file is processed by splitting each line into an array. Each column is formatted by encoding any invalid characters, and ensuring that if the columns type is a string, that the data is wrapped in quotes. After the data processing is complete the file is written to the user's current directory.

When designing this application the first decision to be made was to figure out how to get the attributes data types. We went through two iterations of this design. One used type inference by looking at the actual data of the .csv file trying to guess what data type the column was, and the other required the user to enter each type as an argument when the program was called. We decided on the second option due to the lack of consistency in the .csv files which caused for faulty type inference. Another problem which had to be fixed was dealing with commas within quotes. The .csv files usually denote a string with a comma in it by surrounding the string with quotes. This breaks splitting each column by commas because the algorithm only looked for singular commas. A simple regex selector, found on stackoverflow, which ignores commas inside quotes was used to fix this problem. Finally, WEKA requires all strings to be wrapped in quotes. A simple format column function solved this problem by adding quotes to lines which needed them. Unfortunately a small problem surfaced when this was implemented. Because single quotes were used to wrap the data, any text with a single quote in it (such as i'm, that's, it's) broke the quote and henceforth broke the parse. To solve this, a find and replace is done on the data replacing any single quotes with their escaped counterpart: \textbackslash'.

\subsection{UML}
\begin{center}
  \begin{tikzpicture}
    \begin{class}[text width=10cm ]{ArffConvert}{0, 0}
      \operation{+ convert()}
      \operation{+ processArgs() : Promise}
      \operation{+ processData(fileName : string, types : Array) : string}
      \operation{- askQuestion(question : string) : Promies}
      \operation{- formatStringColumnData(str : string) : string}
      \operation{- stripWrappedQuotes(str : string) : string}
      \operation{- escapeInvalidStringCharacters(str : string) : string}
      \operation{- splitIgnoreCommaInQuotes(str : string) : Array}
    \end{class}
  \end{tikzpicture}
\end{center}

\subsubsection{convert()}
Convert is the function main call. It connects methods together, reads the .csv file, formats and then creates the new .arff file.

\subsubsection{processArgs()}
processArgs parses each command line argument passed in. It first extracts the fileName argument from the argsArray, and then proceeds to loop through each of the type arguments checking to see if they are date or enum types. If the type is date or enum the processArgs function asks the user what format or enum options to set that type to be. This function returns a promise to handle the asynchronous call that is used to get the users input.

\subsubsection{processData()}
The processData function call ensures that the data of the arff file is in the correct format. It loops through every line of the .csv file wrapping strings in quotes, and replaces invalid characters. It then returns a string of the formatted file.

\subsubsection{Utility Functions}
There are five private utility functions. Four of them are used to format strings, and the fifth is a simple function to wrap the Node.js readline call.

\section{Algorithm Comparison Experiment Data}

\subsection{Data Sets}
We will use five different data sets to compare the machine learning algorithms.  When deciding what data sets to use, we considered three characteristics: understandability, number of instances, and number of attributes.  By using data sets with clear classifications and data types, it will be much easier to interpret and predict the results.  We wanted a high variance in the number of instances to ascertain how the size of the data affects the success of the algorithms.  Lastly, we wanted a low number of attributes because there is no simple means of generating ARFF attribute names.  The five data sets are a poker hand data set, a king-rook chess end game data set, a connect-4 ending state data set, a tic-tac-toe ending state data set, and a car model data set.

\subsection{Machine Learning Algorithms}
We will be testing five learning algorithms, each will be referenced to a source explaining the algorithm:
\begin{enumerate}
	\item K-nearest neighbor (Larose, 2004)
	\item Na\"{i}ve Bayes (Zhang, 2004)
	\item Logistic regression (Witten, Frank, Hall, \& Pal, 2000)
	\item Decision tree with pruning (Patil, Wadhai, \& Gokhale, 2010)
	\item Support vector machine with a nonlinear kernel (Hsu, Chang, \& Lin, 2010)
\end{enumerate}
These algorithms are already written on Weka, so the only editing of the algorithms will be tuning.

\subsection{Evaluation Measures}
We will evaluate the algorithms using two Weka analysis methods: percent correct and area under ROC curve.  Percent correct is a useful evaluation for understanding the overall accuracy of an algorithm with a single number.  We chose the area under ROC curve because while it is a generally useful performance measure, it also has the advantage of depicting the tradeoff between true positives and false positives (Fawcett, 2005).


\begin{thebibliography}{9}
	\bibitem{patterrec}
	Fawcett, T. (2005). An introduction to ROC analysis. \textit{Pattern Recognition Letters, 27}. 
	
	\bibitem{supportvector}
	Hsu, C.W., Chang, C.C., \& Lin, C.J. (2010). A Practical Guide to Support Vector Classification. 
	
	\bibitem{kneigh}
	Larose, D. T. (2004). K-Nearest Neighbor Algorithm. \textit{Discovering Knowledge in Data: An Introduction to Data Mining, 5}.  (90-106)
	
	\bibitem{decisiontree}
	Patil, D. D., Wadhai, V. M., \& Gokhale, J. A. (2010). Evaluation of Decision Tree Pruning Algorithms for Complexity and Classification Accuracy. \textit{International Journal of Computer Applications}. (Volume 11)
	
	\bibitem{datamining}
	Witten, I. H., Frank, E., Hall, M. A., \& Pal, C. J. (2000). \textit{Data Mining: Practical Machine Learning Tools and Techniques}, 129-131. Elsevier Inc.
	
	\bibitem{niavebayes}
	Zhang, H. (2004). The Optimality of Naive Bayes. \textit{American Association for Artificial Intelligence}.
	 
	
\end{thebibliography}

\end{document}